# TAMER - Handwritten Mathematical Expression Recognition

Dá»± Ã¡n nÃ y triá»ƒn khai mÃ´ hÃ¬nh **TAMER** (Two-way Attention-based Model for Expression Recognition) cho nháº­n dáº¡ng biá»ƒu thá»©c toÃ¡n há»c viáº¿t tay (Handwritten Mathematical Expression - HME).

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#tá»•ng-quan)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [Cáº¥u hÃ¬nh](#cáº¥u-hÃ¬nh)
- [ÄÃ¡nh giÃ¡](#Ä‘Ã¡nh-giÃ¡)
- [Kiáº¿n trÃºc mÃ´ hÃ¬nh](#kiáº¿n-trÃºc-mÃ´-hÃ¬nh)

## ğŸ¯ Tá»•ng quan

TAMER lÃ  mÃ´ hÃ¬nh encoder-decoder sá»­ dá»¥ng:
- **Encoder**: DenseNet-based Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« áº£nh biá»ƒu thá»©c toÃ¡n há»c
- **Decoder**: Transformer-based decoder vá»›i cÆ¡ cháº¿ attention
- **Training**: Bidirectional training (two-way) Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t
- **Inference**: Beam search Ä‘á»ƒ tÃ¬m kiáº¿m chuá»—i LaTeX tá»‘t nháº¥t

Dá»± Ã¡n bao gá»“m 2 phiÃªn báº£n:
- **0-baseline**: PhiÃªn báº£n TAMER gá»‘c
- **1-gat**: PhiÃªn báº£n TAMER tÃ­ch há»£p Graph Attention Network (GAT) Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng nháº­n dáº¡ng

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
ChuyenDe-Tamer/
â”œâ”€â”€ 0-baseline/          # PhiÃªn báº£n TAMER gá»‘c
â”‚   â”œâ”€â”€ config/          # File cáº¥u hÃ¬nh YAML
â”‚   â”œâ”€â”€ eval/            # Scripts Ä‘Ã¡nh giÃ¡
â”‚   â”œâ”€â”€ tamer/           # Package chÃ­nh
â”‚   â”‚   â”œâ”€â”€ model/       # Kiáº¿n trÃºc mÃ´ hÃ¬nh
â”‚   â”‚   â”œâ”€â”€ datamodule/  # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”‚   â””â”€â”€ utils/       # Tiá»‡n Ã­ch
â”‚   â”œâ”€â”€ train.py         # Script training
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ 1-gat/               # PhiÃªn báº£n TAMER vá»›i GAT
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ tamer/
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â””â”€â”€ gat.py   # Graph Attention Network
â”‚   â”‚   â”œâ”€â”€ datamodule/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ KetQua/              # Káº¿t quáº£ vÃ  checkpoints
```

## ğŸ”§ CÃ i Ä‘áº·t

### YÃªu cáº§u

- Python 3.7+
- PyTorch 1.8+
- PyTorch Lightning

### CÃ i Ä‘áº·t dependencies

```bash
# CÃ i Ä‘áº·t cho baseline
cd 0-baseline
pip install -r requirements.txt
pip install -e .

# Hoáº·c cÃ i Ä‘áº·t cho phiÃªn báº£n GAT
cd 1-gat
pip install -r requirements.txt
pip install -e .
```

### CÃ i Ä‘áº·t PyTorch

Äáº£m báº£o cÃ i Ä‘áº·t PyTorch phÃ¹ há»£p vá»›i há»‡ thá»‘ng cá»§a báº¡n:

```bash
# VÃ­ dá»¥ cho CUDA 11.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu111
```

## ğŸš€ Sá»­ dá»¥ng

### Training

Sá»­ dá»¥ng PyTorch Lightning CLI Ä‘á»ƒ training:

```bash
cd 0-baseline  # hoáº·c cd 1-gat

# Training vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
python train.py fit --config config/crohme.yaml

# Training vá»›i cáº¥u hÃ¬nh debug (sá»‘ lÆ°á»£ng máº«u nhá»)
python train.py fit --config config/crohme_debug.yaml
```

### ÄÃ¡nh giÃ¡ (Evaluation)

```bash
cd 0-baseline/eval  # hoáº·c cd 1-gat/eval

# ÄÃ¡nh giÃ¡ trÃªn CROHME
bash eval_crohme.sh

# ÄÃ¡nh giÃ¡ trÃªn HME100K
bash eval_hme100k.sh

# Hoáº·c cháº¡y trá»±c tiáº¿p
python test.py \
    --folder data/crohme \
    --version 0 \
    --test_year 2014 \
    --max_size 320000 \
    --scale_to_limit true
```

### Sá»­ dá»¥ng Jupyter Notebook

CÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c notebook cÃ³ sáºµn:
- `TAMER_Kaggle_Setup.ipynb`: Setup cho Kaggle
- `tamer37_baseline.ipynb`: Notebook training vÃ  evaluation

## âš™ï¸ Cáº¥u hÃ¬nh

CÃ¡c file cáº¥u hÃ¬nh YAML náº±m trong thÆ° má»¥c `config/`:

### Cáº¥u hÃ¬nh mÃ´ hÃ¬nh (model)

```yaml
model:
  d_model: 256              # KÃ­ch thÆ°á»›c embedding
  growth_rate: 24           # Growth rate cho DenseNet encoder
  num_layers: 16            # Sá»‘ lá»›p DenseNet
  nhead: 8                 # Sá»‘ attention heads
  num_decoder_layers: 3    # Sá»‘ lá»›p decoder
  dim_feedforward: 1024    # KÃ­ch thÆ°á»›c feedforward
  dropout: 0.3
  vocab_size: 113          # KÃ­ch thÆ°á»›c vocabulary
  cross_coverage: true     # Cross attention coverage
  self_coverage: true      # Self attention coverage
  beam_size: 10            # Beam search size
  max_len: 150             # Äá»™ dÃ i tá»‘i Ä‘a sequence
```

### Cáº¥u hÃ¬nh dá»¯ liá»‡u (data)

```yaml
data:
  folder: data/crohme      # ThÆ° má»¥c dá»¯ liá»‡u
  test_folder: 2014       # ThÆ° má»¥c test
  max_size: 320000        # KÃ­ch thÆ°á»›c áº£nh tá»‘i Ä‘a
  scale_to_limit: true
  train_batch_size: 8
  eval_batch_size: 2
  num_workers: 5
```

### Cáº¥u hÃ¬nh trainer

```yaml
trainer:
  gpus: 1
  max_epochs: 100
  precision: 16            # Mixed precision training
  check_val_every_n_epoch: 2
  deterministic: true
```

## ğŸ“Š ÄÃ¡nh giÃ¡

MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng **ExpRate** (Expression Rate):
- **ExpRate**: Tá»· lá»‡ biá»ƒu thá»©c Ä‘Æ°á»£c nháº­n dáº¡ng chÃ­nh xÃ¡c hoÃ n toÃ n
- **ExpRate<=1**: Tá»· lá»‡ biá»ƒu thá»©c cÃ³ edit distance <= 1
- **ExpRate<=2**: Tá»· lá»‡ biá»ƒu thá»©c cÃ³ edit distance <= 2

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong:
- `lightning_logs/version_X/`: Logs vÃ  checkpoints
- `errors_YEAR.json`: CÃ¡c lá»—i nháº­n dáº¡ng
- `predictions.json`: Táº¥t cáº£ predictions

## ğŸ—ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh

### TAMER Baseline

```
Input Image â†’ DenseNet Encoder â†’ Positional Encoding
                                    â†“
                            Transformer Decoder
                                    â†“
                            Bidirectional Training
                                    â†“
                            Beam Search â†’ LaTeX Output
```

### TAMER-GAT

PhiÃªn báº£n `1-gat` tÃ­ch há»£p Graph Attention Network vÃ o encoder Ä‘á»ƒ:
- MÃ´ hÃ¬nh hÃ³a quan há»‡ khÃ´ng gian giá»¯a cÃ¡c kÃ½ tá»±
- Cáº£i thiá»‡n kháº£ nÄƒng nháº­n dáº¡ng cÃ¡c biá»ƒu thá»©c phá»©c táº¡p

### CÃ¡c thÃ nh pháº§n chÃ­nh

1. **Encoder** (`tamer/model/encoder.py`):
   - DenseNet-based feature extraction
   - Positional encoding cho áº£nh

2. **Decoder** (`tamer/model/decoder.py`):
   - Transformer decoder vá»›i multi-head attention
   - Coverage mechanism Ä‘á»ƒ trÃ¡nh láº·p láº¡i

3. **Beam Search** (`tamer/utils/beam_search.py`):
   - TÃ¬m kiáº¿m chuá»—i LaTeX tá»‘i Æ°u

4. **Data Module** (`tamer/datamodule/`):
   - Xá»­ lÃ½ dá»¯ liá»‡u CROHME vÃ  HME100K
   - Chuyá»ƒn Ä‘á»•i LaTeX sang Ground Truth Data (GTD)

## ğŸ“š Datasets

Dá»± Ã¡n há»— trá»£:
- **CROHME**: Competition on Recognition of Online Handwritten Mathematical Expressions
- **HME100K**: Large-scale handwritten math expression dataset

## ğŸ” CÃ¡c tÃ­nh nÄƒng

- âœ… Bidirectional training
- âœ… Coverage mechanism (cross & self)
- âœ… Beam search inference
- âœ… Mixed precision training (FP16)
- âœ… Multi-GPU support (DDP)
- âœ… Graph Attention Network (phiÃªn báº£n GAT)

## ğŸ“ Ghi chÃº

- Checkpoints Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng trong `lightning_logs/`
- Model checkpoint tá»‘t nháº¥t Ä‘Æ°á»£c chá»n dá»±a trÃªn `val_ExpRate`
- Sá»­ dá»¥ng seed=7 Ä‘á»ƒ Ä‘áº£m báº£o reproducibility



