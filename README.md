# TAMER - Handwritten Mathematical Expression Recognition

Dá»± Ã¡n nÃ y triá»ƒn khai mÃ´ hÃ¬nh **TAMER** (Two-way Attention-based Model for Expression Recognition) cho nháº­n dáº¡ng biá»ƒu thá»©c toÃ¡n há»c viáº¿t tay (Handwritten Mathematical Expression - HME).

**TÃ¡c giáº£:** Phan HoÃ ng Kháº£i  
**ÄÆ¡n vá»‹:** Äáº¡i há»c SÆ° pháº¡m Ká»¹ thuáº­t TPHCM (HCMUTE)

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#tá»•ng-quan)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [Deep Dive: Graph Attention Networks (GAT)](#deep-dive-graph-attention-networks-gat)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [Cáº¥u hÃ¬nh](#cáº¥u-hÃ¬nh)
- [Káº¿t quáº£](#káº¿t-quáº£)

## ğŸ¯ Tá»•ng quan

TAMER lÃ  má»™t kiáº¿n trÃºc máº¡nh máº½ káº¿t há»£p giá»¯a CNN vÃ  Transformer Ä‘á»ƒ chuyá»ƒn Ä‘á»•i hÃ¬nh áº£nh biá»ƒu thá»©c toÃ¡n há»c viáº¿t tay thÃ nh chuá»—i LaTeX. Dá»± Ã¡n nÃ y bao gá»“m hai phiÃªn báº£n chÃ­nh:

1.  **0-baseline**: PhiÃªn báº£n chuáº©n sá»­ dá»¥ng DenseNet lÃ m Encoder vÃ  Transformer lÃ m Decoder.
2.  **1-gat**: PhiÃªn báº£n nÃ¢ng cáº¥p tÃ­ch há»£p **Graph Attention Networks (GAT)** vÃ o bá»™ mÃ£ hÃ³a (Encoder) Ä‘á»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian vÃ  cáº¥u trÃºc cá»§a biá»ƒu thá»©c.

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
ChuyenDe-Tamer/
â”œâ”€â”€ 0-baseline/          # PhiÃªn báº£n TAMER gá»‘c (DenseNet + Transformer)
â”œâ”€â”€ 1-gat/               # PhiÃªn báº£n nÃ¢ng cao (DenseNet + GAT + Transformer)
â”‚   â”œâ”€â”€ tamer/
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â”œâ”€â”€ gat.py   # CÃ i Ä‘áº·t lá»›p Graph Attention
â”‚   â”‚   â”‚   â””â”€â”€ encoder.py # Encoder tÃ­ch há»£p GAT
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ KetQua/              # LÆ°u trá»¯ káº¿t quáº£ thá»±c nghiá»‡m
â””â”€â”€ README.md            # TÃ i liá»‡u dá»± Ã¡n
```

## ğŸ§  Deep Dive: Graph Attention Networks (GAT)

Äiá»ƒm nháº¥n cá»§a dá»± Ã¡n nÃ y lÃ  viá»‡c tÃ­ch há»£p **Graph Attention Networks (GAT)** vÃ o kiáº¿n trÃºc Encoder. DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch chi tiáº¿t ká»¹ thuáº­t vá» cÃ¡ch GAT hoáº¡t Ä‘á»™ng trong bÃ i toÃ¡n nÃ y:

### Táº¡i sao láº¡i dÃ¹ng GAT?

CÃ¡c máº¡ng CNN truyá»n thá»‘ng (nhÆ° DenseNet) ráº¥t giá»i trong viá»‡c trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cá»¥c bá»™ (local features). Tuy nhiÃªn, Ä‘á»‘i vá»›i biá»ƒu thá»©c toÃ¡n há»c, má»‘i quan há»‡ giá»¯a cÃ¡c kÃ½ tá»± khÃ´ng chá»‰ náº±m á»Ÿ vá»‹ trÃ­ lÃ¢n cáº­n mÃ  cÃ²n phá»¥ thuá»™c vÃ o cáº¥u trÃºc ngá»¯ nghÄ©a 2D (vÃ­ dá»¥: phÃ¢n sá»‘, sá»‘ mÅ©, chá»‰ sá»‘ dÆ°á»›i).

GAT cho phÃ©p mÃ´ hÃ¬nh coi báº£n Ä‘á»“ Ä‘áº·c trÆ°ng (feature map) nhÆ° má»™t Ä‘á»“ thá»‹, nÆ¡i má»—i Ä‘iá»ƒm áº£nh (pixel) hoáº·c vÃ¹ng Ä‘áº·c trÆ°ng lÃ  má»™t nÃºt (node). CÆ¡ cháº¿ Attention giÃºp má»—i nÃºt cÃ³ thá»ƒ "táº­p trung" (attend) vÃ o cÃ¡c nÃºt lÃ¢n cáº­n quan trá»ng nháº¥t Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin, thay vÃ¬ nhÃ¢n cháº­p cá»‘ Ä‘á»‹nh nhÆ° CNN.

### Kiáº¿n trÃºc chi tiáº¿t (Implementation Details)

Module GAT Ä‘Æ°á»£c cÃ i Ä‘áº·t trong `1-gat/tamer/model/gat.py` vÃ  `1-gat/tamer/model/encoder.py`.

1.  **XÃ¢y dá»±ng Äá»“ thá»‹ (Graph Construction)**:
    *   Feature map Ä‘áº§u ra tá»« DenseNet cÃ³ kÃ­ch thÆ°á»›c `[H, W, D]`.
    *   Ta biáº¿n Ä‘á»•i feature map nÃ y thÃ nh má»™t lÆ°á»›i Ä‘á»“ thá»‹ (grid graph) vá»›i `N = H * W` nÃºt.
    *   **Adjacency Matrix**: XÃ¢y dá»±ng ma tráº­n ká» dá»±a trÃªn káº¿t ná»‘i 4 hÆ°á»›ng (4-connectivity: trÃªn, dÆ°á»›i, trÃ¡i, pháº£i). Má»—i nÃºt Ä‘Æ°á»£c káº¿t ná»‘i vá»›i 4 nÃºt lÃ¢n cáº­n cá»§a nÃ³.

2.  **CÆ¡ cháº¿ GAT Layer**:
    *   Má»—i lá»›p GAT (`GATLayer`) sá»­ dá»¥ng **Multi-head Attention**.
    *   Äáº§u vÃ o lÃ  cÃ¡c features cá»§a nÃºt $h_i$.
    *   Há»‡ sá»‘ attention $e_{ij}$ giá»¯a nÃºt $i$ vÃ  nÃºt lÃ¢n cáº­n $j$ Ä‘Æ°á»£c tÃ­nh toÃ¡n thÃ´ng qua má»™t máº¡ng nÆ¡-ron truyá»n tháº³ng (feed-forward neural network):
        $$e_{ij} = \text{LeakyReLU}(\vec{a}^T [W\vec{h}_i || W\vec{h}_j])$$
    *   Há»‡ sá»‘ nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c chuáº©n hÃ³a báº±ng Softmax Ä‘á»ƒ táº¡o ra trá»ng sá»‘ $\alpha_{ij}$.
    *   Äáº§u ra cá»§a nÃºt $i$ lÃ  tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c nÃºt lÃ¢n cáº­n:
        $$\vec{h}'_i = \sigma(\sum_{j \in \mathcal{N}_i} \alpha_{ij} W\vec{h}_j)$$

3.  **TÃ­ch há»£p vÃ o Encoder**:
    *   Quy trÃ¬nh xá»­ lÃ½: `Image -> DenseNet -> Feature Map -> Flatten -> GAT Layers -> Reshape -> Feature Map -> Positional Encoding -> Transformer Decoder`.
    *   Viá»‡c chÃ¨n GAT vÃ o giá»¯a DenseNet vÃ  Transformer giÃºp lÃ m giÃ u feature map vá»›i thÃ´ng tin ngá»¯ cáº£nh cáº¥u trÃºc trÆ°á»›c khi giáº£i mÃ£.

## ğŸ”§ CÃ i Ä‘áº·t

YÃªu cáº§u mÃ´i trÆ°á»ng:
- Python 3.7+
- PyTorch 1.8+
- CUDA (náº¿u dÃ¹ng GPU)

CÃ i Ä‘áº·t cÃ¡c gÃ³i phá»¥ thuá»™c:

```bash
# CÃ i Ä‘áº·t cho phiÃªn báº£n GAT (KhuyÃªn dÃ¹ng)
cd 1-gat
pip install -r requirements.txt
pip install -e .
```

Náº¿u muá»‘n cháº¡y baseline:
```bash
cd 0-baseline
pip install -r requirements.txt
pip install -e .
```

## ï¿½ Sá»­ dá»¥ng

### QuÃ¡ trÃ¬nh Huáº¥n luyá»‡n (Training)

Äá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh, sá»­ dá»¥ng script `train.py`. Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i cáº¥u hÃ¬nh trong thÆ° má»¥c `config/`.

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c source code
cd 1-gat

# Cháº¡y huáº¥n luyá»‡n vá»›i file config máº·c Ä‘á»‹nh
python train.py fit --config config/crohme.yaml

# Debug nhanh vá»›i dá»¯ liá»‡u nhá»
python train.py fit --config config/crohme_debug.yaml
```

### ÄÃ¡nh giÃ¡ (Evaluation)

Sá»­ dá»¥ng cÃ¡c script trong thÆ° má»¥c `eval/` Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n.

```bash
cd 1-gat/eval

# ÄÃ¡nh giÃ¡ trÃªn táº­p dá»¯ liá»‡u CROHME
bash eval_crohme.sh
```

## âš™ï¸ Cáº¥u hÃ¬nh

CÃ¡c tham sá»‘ quan trá»ng trong `config/crohme.yaml`:

- **model**:
    - `d_model`: 256 (KÃ­ch thÆ°á»›c vector Ä‘áº·c trÆ°ng)
    - `use_gat`: true (Báº­t táº¯t module GAT)
    - `gat_num_layers`: 2 (Sá»‘ lá»›p GAT chá»“ng lÃªn nhau)
    - `gat_num_heads`: 8 (Sá»‘ Ä‘áº§u attention trong GAT)
- **data**:
    - `folder`: ÄÆ°á»ng dáº«n Ä‘áº¿n dá»¯ liá»‡u áº£nh
    - `batch_size`: KÃ­ch thÆ°á»›c batch

## ï¿½ Káº¿t quáº£

Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c Ä‘o báº±ng **Expression Rate (ExpRate)**. PhiÃªn báº£n tÃ­ch há»£p GAT Ä‘Æ°á»£c ká»³ vá»ng sáº½ xá»­ lÃ½ tá»‘t hÆ¡n cÃ¡c trÆ°á»ng há»£p biá»ƒu thá»©c cÃ³ cáº¥u trÃºc phá»©c táº¡p hoáº·c nÃ©t viáº¿t chá»“ng chÃ©o nhá» kháº£ nÄƒng lan truyá»n tin (message passing) linh hoáº¡t cá»§a máº¡ng Ä‘á»“ thá»‹.

---
Â© 2025 Phan HoÃ ng Kháº£i - Äáº¡i há»c SÆ° pháº¡m Ká»¹ thuáº­t TPHCM (HCMUTE).
